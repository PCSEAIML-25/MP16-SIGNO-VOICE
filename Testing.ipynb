{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import sklearn\n",
    "#import tensorflow\n",
    "\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM ,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #makes detections\n",
    "mp_drawing = mp.solutions.drawing_utils #draws detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_new_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False   #image not writeable\n",
    "    results = model.process(image)  #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                                                                            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    pose=pose[:69]\n",
    "    return np.concatenate([pose,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_Path = os.path.join(\"test video_all/test video\")\n",
    "Data_Path = os.path.join(\"test video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([\"Book\",\"Do\",\"Eat\",\"Go\",\"Good\",\"Hello\",\"Home\",\"Hungry\",\"I\",\"Morning\",\"No\",\"Not\",\"Pizza\" , \"Place\" ,\"Read\",\"School\",\"Student\",\"Teacher\",\"Thank You\", \"This\" , \"Tomorrow\" ,\"Want\", \"What\", \"Yes\", \"Yesterday\",\"You\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num , label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in range(length):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(Data_Path, (str(sequence))))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emotion detection part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Landmark indices\n",
    "RIGHT_INNER_EYELID = 133 \n",
    "LEFT_INNER_EYELID = 362   \n",
    "\n",
    "right_inner_eyebrow = 55\n",
    "left_inner_eyebrow = 285\n",
    "\n",
    "RIGHT_EYE_OUTER = 33   # Right outer eye corner\n",
    "LEFT_EYE_OUTER = 263   # Left outer eye corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_emotion(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for face_landmarks in face_results.multi_face_landmarks:\n",
    "            # Draw face mesh landmarks\n",
    "            \"\"\" mp_drawing.draw_landmarks(\n",
    "                frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "            ) \"\"\"\n",
    "\n",
    "            # Extract image dimensions\n",
    "            h, w, _ = frame.shape\n",
    "\n",
    "            # Function to get pixel coordinates\n",
    "            def get_pixel_coords(landmark):\n",
    "                return int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "            # Get the upper eyelid points\n",
    "            x1_right, y1_right = get_pixel_coords(face_landmarks.landmark[RIGHT_INNER_EYELID])\n",
    "            x1_left, y1_left = get_pixel_coords(face_landmarks.landmark[LEFT_INNER_EYELID])\n",
    "\n",
    "            x2_right, y2_right = get_pixel_coords(face_landmarks.landmark[right_inner_eyebrow])\n",
    "            x2_left, y2_left = get_pixel_coords(face_landmarks.landmark[left_inner_eyebrow])\n",
    "\n",
    "            # Calculate Euclidean distance\n",
    "            dist_right = np.linalg.norm(np.array([x1_right, y1_right]) - np.array([x2_right, y2_right]))\n",
    "            dist_left = np.linalg.norm(np.array([x1_left, y1_left]) - np.array([x2_left, y2_left]))\n",
    "\n",
    "            # Get outer eye corners\n",
    "            x_r_eye, y_r_eye = get_pixel_coords(face_landmarks.landmark[RIGHT_EYE_OUTER])\n",
    "            x_l_eye, y_l_eye = get_pixel_coords(face_landmarks.landmark[LEFT_EYE_OUTER])\n",
    "\n",
    "            # Compute inter-eye distance (used for normalization)\n",
    "            eye_distance = np.linalg.norm(np.array([x_r_eye, y_r_eye]) - np.array([x_l_eye, y_l_eye]))\n",
    "\n",
    "            # Normalize distances\n",
    "            norm_dist_right = dist_right / eye_distance\n",
    "            norm_dist_left = dist_left / eye_distance\n",
    "\n",
    "            # Check if the person is frowning\n",
    "            if (norm_dist_right <= 0.16) and (norm_dist_left <= 0.16):\n",
    "                return (\"?\")\n",
    "            else:\n",
    "                return (\".\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "frown_count = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "    for sequence in range(length):\n",
    "        \n",
    "        for frame_num in range(30):\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if sequence == (length-1):\n",
    "                emotion = face_emotion(frame)\n",
    "                if emotion == \"?\":\n",
    "                    frown_count += 1\n",
    "                \n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "                            \n",
    "            draw_landmarks(image, results)\n",
    "                        \n",
    "            if frame_num == 0: \n",
    "                cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'Collecting Video Number {}'.format(sequence), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(1500)\n",
    "            else: \n",
    "                cv2.putText(image, 'Collecting Video Number {}'.format(sequence), (15,12), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.imshow('OpenCV Feed', image)               \n",
    "            \n",
    "            keypoints = extract_keypoints(results)\n",
    "\n",
    "            npy_path = os.path.join(Data_Path, (str(sequence)), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if frown_count > 15:\n",
    "    suffix = \"?\"\n",
    "else:\n",
    "    suffix = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(frown_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints, center_keypoint, reference_distance):\n",
    "    # Reshape the keypoints into (x, y, z) coordinates\n",
    "    #print(keypoints)\n",
    "    keypoints = keypoints.reshape(-1, 3)\n",
    "    \n",
    "    #print(keypoints)\n",
    "    # Subtract center keypoint to get relative coordinates\n",
    "    relative_keypoints = keypoints - center_keypoint\n",
    "\n",
    "    #print(relative_keypoints)\n",
    "    \n",
    "    # If reference distance is provided, scale the keypoints\n",
    "    relative_keypoints = relative_keypoints / reference_distance\n",
    "    #relative_keypoints = keypoints / reference_distance\n",
    "    \n",
    "\n",
    "    return relative_keypoints.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "\n",
    "for sequence in range(length):\n",
    "    window = []\n",
    "    for frame_num in range(30):\n",
    "        frame = np.load(os.path.join(Data_Path,str(sequence), f\"{frame_num}.npy\"))\n",
    "        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "        left_shoulder = frame[11*3:11*3+3]\n",
    "        right_shoulder = frame[12*3:12*3+3]\n",
    "        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "        if not reference_distance:\n",
    "            reference_distance=1\n",
    "        \n",
    "\n",
    "        frame = normalize_keypoints(frame, center_keypoint, reference_distance)\n",
    "        window.append(frame)\n",
    "    sequences.append(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter1d  # For temporal smoothing\n",
    "\n",
    "# Function to calculate relative hand keypoints\n",
    "def preprocess_hand_keypoints(hand_keypoints):\n",
    "    # If there are hand keypoints, calculate relative positions with respect to the wrist (0th keypoint)\n",
    "    if np.any(hand_keypoints):\n",
    "        wrist_keypoint = hand_keypoints[0:3]  # Wrist is the first keypoint in MediaPipe\n",
    "        relative_hand_keypoints = (hand_keypoints.reshape(-1, 3) - wrist_keypoint)  # Relative to wrist\n",
    "    else:\n",
    "        relative_hand_keypoints = np.zeros(21 * 3)  # If no hand keypoints, return zeros\n",
    "    return relative_hand_keypoints.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Preprocessing: Including relative hand keypoints and temporal smoothing\n",
    "sequences, labels = [], []\n",
    "\n",
    "\n",
    "for sequence in range(length):\n",
    "    window = []\n",
    "    for frame_num in range(30):\n",
    "        frame = np.load(os.path.join(Data_Path,str(sequence), f\"{frame_num}.npy\"))\n",
    "        \n",
    "        # Center keypoint (nose) and shoulder distance (for normalization)\n",
    "        center_keypoint = frame[0:3]  # Nose keypoint (x, y, z)\n",
    "        left_shoulder = frame[11*3:11*3+3]\n",
    "        right_shoulder = frame[12*3:12*3+3]\n",
    "        reference_distance = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "        if not reference_distance:\n",
    "            reference_distance = 1\n",
    "        \n",
    "        # Normalize the pose keypoints relative to the nose\n",
    "        normalized_pose = normalize_keypoints(frame[:69], center_keypoint, reference_distance)\n",
    "        \n",
    "        # Preprocess left hand keypoints (relative to wrist)\n",
    "        left_hand = frame[69:69 + 21*3]\n",
    "        relative_left_hand = preprocess_hand_keypoints(left_hand)\n",
    "        \n",
    "        # Preprocess right hand keypoints (relative to wrist)\n",
    "        right_hand = frame[69 + 21*3:]\n",
    "        relative_right_hand = preprocess_hand_keypoints(right_hand)\n",
    "        \n",
    "        # Concatenate normalized pose, relative left hand, and relative right hand keypoints\n",
    "        full_frame = np.concatenate([normalized_pose, relative_left_hand, relative_right_hand])\n",
    "        \n",
    "        window.append(full_frame)\n",
    "    \n",
    "    # Convert the window into a numpy array for smoothing\n",
    "    window = np.array(window)\n",
    "    \n",
    "    # Apply temporal smoothing using a moving average filter\n",
    "    smoothed_window = uniform_filter1d(window, size=3, axis=0)\n",
    "    \n",
    "    sequences.append(smoothed_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30, 195)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(sequences).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You 0.9990669\n",
      "Want 0.8951767\n",
      "Eat 0.9998957\n",
      "YOU WANT EAT?\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\n",
    "\n",
    "for sign in range(length):\n",
    "    res = model.predict(np.expand_dims(sequences[sign], axis=0))\n",
    "    if sign == length-1:\n",
    "        raw = raw + str(actions[np.argmax(res)])\n",
    "    else:\n",
    "        raw = raw + str(actions[np.argmax(res)]) + \" \"\n",
    "    raw = raw.upper()\n",
    "    print(actions[np.argmax(res)] , np.max(res))\n",
    "    #print(res)\n",
    "raw = raw + suffix\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: 'Do you want to eat?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\tbase_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "\tapi_key=\"\"                                                  #enter api key\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI skilled at translating raw sign language input into grammatically correct English sentences. Remember that when a word is repeated twice, it means that the word is in plural form not that it is 2 in quantity.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sign language into proper English sentences.\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'HOME RAIN HEAVY.'\\nTranslation: 'It is raining heavily in my home area.'\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'I TOMORROW EAT FRUIT FRUIT.'\\nTranslation: 'Tomorrow I will eat fruits.'\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'CLASS STUDENTS SIT.'\\nTranslation: 'There are students sitting in the class.'\"},\n",
    "    \n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'I TONIGHT HOME GO LATE.'\\nTranslation: 'I will go home late tonight.'\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \"Raw Input: 'YOU HUNGRY?'\\nTranslation: 'Are you feeling hungry?'\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": f\"Raw Input: {raw}\"},\n",
    "]\n",
    "\n",
    "\n",
    "llm = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=20\n",
    ")\n",
    "\n",
    "final = str((llm.choices[0].message.content))\n",
    "print(final)\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "import os\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "\n",
    "myobj = gTTS(text=final, lang=language, slow=False)\n",
    "\n",
    "\n",
    "myobj.save(\"welcome.mp3\")\n",
    "\n",
    "\n",
    "os.system(\"start welcome.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
